---
title: "Basic Monitoring"
description: "Learn how to track and monitor your LLM interactions using the Basalt SDK."
---

<Info>
  Basic monitoring is only available for prompts retrieved directly from Basalt using the `get` method. This approach is designed for simple use cases where you're using a single prompt and not complex workflows. For advanced monitoring scenarios, please refer to the [Tracing](/monitoring/tracing) documentation.
</Info>

## Basic Usage

When you retrieve a prompt using the SDK, it automatically includes a `generation` object that can be used to monitor the complete interaction. Here's how to use it:

<CodeGroup>

```typescript TypeScript
// Step 1: Get the prompt with monitoring enabled
const { value, generation } = await basalt.prompt.get('my-prompt-slug')

// Step 2: Use the prompt with your LLM provider
const output = await myLLMProvider.complete(value.text)

// Step 3: Record the completion by calling end() with the output
generation.end(output)
```


```python Python
# Step 1: Get the prompt with monitoring enabled
error, result, generation = await basalt.prompt.get('my-prompt-slug')

# Step 2: Use the prompt with your LLM provider
output = await my_llm_provider.complete(value.text)

# Step 3: Record the completion by calling end() with the output
generation.end(output)
```

</CodeGroup>

This simple pattern automatically tracks:

- The prompt that was used
- When the prompt was retrieved
- When the completion was received
- The full completion text
- The total time taken for the interaction

## User and Organization Identification

You can associate the generation with a specific user and organization by using the `identify` method:

<CodeGroup>

```typescript TypeScript
// Identify the user and organization for this generation
generation.trace.identify({
  user: {
    id: 'user@example.com',
    name: 'User Name'
  },
  organization: {
    id: 'org_123',
    name: 'Organization Name'
  }
})
```


```python Python
# Identify the user and organization for this generation
generation.trace.identify({
  'user': {
    'id': 'user@example.com',
    'name': 'User Name'
  },
  'organization': {
    'id': 'org_123',
    'name': 'Organization Name'
  }
})
```

</CodeGroup>

## Evaluation

You can add automatic evaluators to assess the quality of the LLM output:

<CodeGroup>

```typescript TypeScript
// Add an evaluator to automatically assess the response quality
generation.addEvaluator({
  slug: 'good-answer'
})
```


```python Python
# Add an evaluator to automatically assess the response quality
generation.add_evaluator({
  'slug': 'good-answer'
})
```

</CodeGroup>

Evaluators run automatically when `generation.end()` is called. For more information on available evaluators and how to create custom ones, see the [Evaluation](/monitoring/evaluation) documentation.

### Evaluation Configuration

By default, evaluations are run on 10% of logs. You can configure the evaluation sample rate to control how often evaluations are run.

<CodeGroup>

```typescript TypeScript
// Configure evaluations at the trace level
generation.trace.setEvaluationConfig({
  sampleRate: 0.5  // Run evaluations on 50% of traces
})
```


```python Python
# Configure evaluations at the trace level
generation.trace.set_evaluation_config({
  'sample_rate': 0.5  # Run evaluations on 50% of traces
})
```

</CodeGroup>

## Complete Example

Here's a complete example showing how to use basic monitoring with variables, user identification, and automatic evaluation:

<CodeGroup>

```typescript TypeScript
const result = await basalt.prompt.get('comment-classifier', {
  variables: {
    comment: 'I would like to contribute to this issue'
  }
})

if (result.error) {
  console.error(result.error)
  return
}

const { generation } = result

// Identify the user and organization
generation.trace.identify({
  user: {
    id: 'guillaume@getbasalt.ai',
    name: 'Guillaume Marquis'
  },
  organization: {
    id: 'org_123',
    name: 'Basalt'
  }
})

// Add an automatic evaluator
generation.trace.addEvaluator({
  slug: 'good-answer'
})

// Call your LLM provider (commented out in this example)
// const completion = await openai.chat.completions.create({
//   model: 'gpt-4o',
//   messages: [{ role: 'user', content: 'hey' }]
// })

// Record the LLM output
generation.end('output')
```


```python Python
result = await basalt.prompt.get('comment-classifier', {
  'variables': {
    'comment': 'I would like to contribute to this issue'
  }
})

if result.error:
  print(result.error)
  return

generation = result.generation

# Identify the user and organization
generation.trace.identify({
  'user': {
    'id': 'guillaume@getbasalt.ai',
    'name': 'Guillaume Marquis'
  },
  'organization': {
    'id': 'org_123',
    'name': 'Basalt'
  }
})

# Add an automatic evaluator
generation.trace.add_evaluator({
  'slug': 'good-answer'
})

# Call your LLM provider (commented out in this example)
# completion = openai.chat.completions.create(
#   model='gpt-4o',
#   messages=[{'role': 'user', 'content': 'hey'}]
# )

# Record the LLM output
generation.end('output')
```

</CodeGroup>

## Advanced Use Cases

Basic monitoring is suitable for simple scenarios where you're using a single prompt. For more advanced use cases, such as:

- Tracking multi-step workflows
- Monitoring prompts not stored in Basalt
- Collecting detailed metrics
- Creating custom traces
- Building complex evaluation pipelines

Please refer to the [Tracing](/monitoring/tracing) documentation, which provides comprehensive tools for these scenarios.