---
title: Experiments
---

# Experiments

Experiments enable you to track A/B tests, model comparisons, and feature variations in your AI applications. They provide a structured way to compare different approaches and analyze their performance through observability traces.

<Info>
This section covers the concept of experiments and how they are used for evaluation and comparison. For SDK usage, see the Python or TypeScript tabs.
</Info>

Experiments allow you to:
- Track different variants of prompts, models, or approaches
- Compare performance across versions
- Associate traces with specific experiments
- Attach experiment metadata to observability spans
- Run systematic tests and comparisons
