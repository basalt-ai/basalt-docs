---
title: "Caching Policy"
description: "How Basalt caches prompts for performance and reliability."
---

# Caching Policy

Basalt uses caching at multiple levels to make prompt retrieval **fast**, **reliable**, and **resilient to outages**.

This page describes how caching works for prompts, how it interacts with the Python SDK, and how to tune it for development and production.

## Caching Architecture

Caching happens in two main layers:

1. **Backend API cache**  
   Basalt’s backend keeps a distributed cache of prompt resolutions. This:
   - Reduces latency for repeated lookups
   - Shields your app from transient database or storage issues
   - Helps keep the API responsive under load

2. **SDK local cache**  
   The SDK maintains an in-process cache of recently retrieved prompts. This:
   - Avoids unnecessary network round-trips
   - Improves tail latency, especially in high-QPS services
   - Provides a fallback when the API is temporarily unreachable

You don’t need to manage these layers manually for typical usage, but understanding them helps when tuning behavior.

## Default Behavior

By default, the SDK’s local cache is **enabled** with a short time-to-live (TTL), typically around **5 minutes** per entry. In practice this means:

- Repeated calls like `basalt.prompts.get_sync(slug="welcome-message", tag="production")`
  within the TTL are served from the local cache.
- After the TTL expires, the SDK refreshes the entry by calling the Basalt API.
- The API itself may serve cached data if the underlying prompt has not changed.

The cache key usually includes:

- `slug`
- `tag` (if provided)
- `version` (if provided)

So `slug="greeting", tag="production"` and `slug="greeting", tag="latest"` are cached separately.

## Disabling Cache Per Request

For time-sensitive operations—or when you’re actively editing prompts—you may want to bypass the local cache for a specific call.

In the Python SDK, you can usually control this via a `cache` parameter on `get`:

```python
from basalt import Basalt

basalt = Basalt(api_key="your-api-key")

prompt = basalt.prompts.get_sync(
    slug="welcome-message",
    tag="latest",
    cache=False,  # bypass local cache for this call
)
```

This forces the SDK to hit the Basalt API and refresh its view of the prompt, regardless of what is currently cached locally.

> Note  
> The local cache primarily applies to `get` operations. Listing and describing prompts rely more on the backend cache and may not expose the same per-request cache toggle.

## Global Cache Configuration

You can also control caching behavior at client initialization time. This is useful when you want different policies
for development vs. production.

```python
import os
from basalt import Basalt

is_dev = os.environ.get("ENVIRONMENT") == "development"

# Disable cache entirely in development
basalt = Basalt(
    api_key="your-api-key",
    cache_enabled=not is_dev,
)
```

Some SDKs also allow customizing cache duration:

```python
basalt = Basalt(
    api_key="your-api-key",
    cache_enabled=True,
    cache_duration=60,  # seconds
)
```

Check the language-specific SDK docs for the exact configuration options available.

## Cache Invalidation

The SDK handles most cache invalidation for you:

- Entries expire automatically when their TTL elapses.
- Calls with `cache=False` bypass the cache and refresh the stored value.
- When you change prompts (for example, by publishing new versions), the backend cache invalidates affected entries, and the SDK picks up the new data on the next refresh.

For advanced use cases, you may also have access to explicit cache controls:

```python
# Clear all cached prompts in this process
basalt.cache.clear()

# Clear cache for a specific prompt
basalt.cache.invalidate("prompt", "welcome-message")

# Clear cache for a logical feature or category
basalt.cache.invalidate("feature", "onboarding")
```

Use these sparingly—most applications never need manual invalidation.

## Best Practices

Some practical guidelines for using caching effectively:

- **Production services**
  - Keep caching **enabled** to minimize latency and reduce API calls.
  - Use tags like `production` and `staging` so you can roll out new versions without disabling cache.

- **Development and staging**
  - Consider disabling cache globally, or use `cache=False` on critical calls, so you always see the latest prompt.
  - This is especially helpful when you’re iterating quickly on prompt content.

- **CI/CD and tests**
  - Prefer **deterministic** behavior: pin prompts by `version` and consider disabling cache where reproducibility is essential.

- **High-throughput workloads**
  - Rely on caching when many requests hit the same prompt in a short time window. This reduces rate limit pressure and improves p99 latency.

## Benefits of Caching

Effective use of caching provides:

- **Improved performance** – Faster reads for frequently accessed prompts.
- **Reduced API usage** – Fewer network requests and lower operational cost.
- **Resilience to outages** – Ability to serve recently cached prompts when the API or network is degraded.
- **Smoother rollouts** – Combined with tags, caching lets you promote new versions gradually without impacting latency.

## When to Avoid or Bypass Caching

There are some situations where disabling or bypassing caching is the right choice:

- You are **actively editing** prompts and want each call to reflect the latest changes.
- You are running **integration tests** or CI jobs that must be fully reproducible.
- The prompt is **extremely time-sensitive**, and stale content would be incorrect or unsafe.

In those cases, use `cache=False` where needed, or configure your client to disable caching in that environment.

